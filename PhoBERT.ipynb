{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "PhoBERT.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "rBQzaX7n6N8Y"
      },
      "source": [
        "from google.colab import drive  \n",
        "drive.mount(\"/content/drive\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FfxFm4bK6VEx"
      },
      "source": [
        "%cd drive/MyDrive/Project DeepLearning/"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SCjmX4zTCkRK"
      },
      "source": [
        "## Setup\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q-YbjCkzw0yU"
      },
      "source": [
        "# A dependency of the preprocessing for BERT inputs\n",
        "!pip install -q -U tensorflow-text"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b-P1ZOA0FkVJ"
      },
      "source": [
        "!pip install -q tf-models-official"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q2cE9jPYAtee"
      },
      "source": [
        "!pip install vncorenlp"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7AUSIv4d6jG-"
      },
      "source": [
        "pip install transformers"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d6P4FKZqSZDa"
      },
      "source": [
        "IMPORT THƯ VIỆN\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_XgTpm9ZxoN9"
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import xlrd\n",
        "import torch\n",
        "import pickle\n",
        "from vncorenlp import VnCoreNLP\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score, confusion_matrix, plot_confusion_matrix,ConfusionMatrixDisplay\n",
        "\n",
        "# Load Huggingface transformers\n",
        "from transformers import *\n",
        "\n",
        "# Then what you need from tensorflow.keras\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.layers import Input, Dropout, Dense, LSTM\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.optimizers import *\n",
        "from tensorflow.keras.callbacks import EarlyStopping\n",
        "from tensorflow.keras.initializers import TruncatedNormal\n",
        "from tensorflow.keras.losses import CategoricalCrossentropy\n",
        "from tensorflow.keras.metrics import CategoricalAccuracy\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "from keras.preprocessing.sequence import pad_sequences"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vHDxU6mK_gBv"
      },
      "source": [
        "device_name = tf.test.gpu_device_name()\n",
        "if device_name != '/device:GPU:0':\n",
        "  raise SystemError('GPU device not found')\n",
        "print('Found GPU at: {}'.format(device_name))\n",
        "tf.device('/device:GPU:0')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8AZYTHq6whWT"
      },
      "source": [
        "SETUP BERT"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wOfThOVR7EFG"
      },
      "source": [
        "#######################################\n",
        "### --------- Setup BERT ---------- ###\n",
        "# Name of the BERT model to use\n",
        "model_name = 'vinai/phobert-base'\n",
        "\n",
        "# Load transformers config and set output_hidden_states to False\n",
        "config = RobertaConfig.from_pretrained(model_name)\n",
        "config.output_hidden_states = False\n",
        "# Load BERT tokenizer\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name, config = config)\n",
        "# Load the Transformers BERT model\n",
        "transformer_model = TFAutoModel.from_pretrained(model_name, config = config)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wWvg_QPf0yqf"
      },
      "source": [
        "MAX_SEQ_LEN = 250\n",
        "number_topic =12"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sb1d8H8JCp4Q"
      },
      "source": [
        "def seq2inds(x) :\n",
        "    x = tokenizer(\n",
        "        text= x ,\n",
        "        add_special_tokens=True,\n",
        "        max_length=MAX_SEQ_LEN,\n",
        "        truncation=True,\n",
        "        padding=True, \n",
        "        return_tensors='tf',\n",
        "        return_token_type_ids = False,\n",
        "        return_attention_mask = False,\n",
        "        verbose = True)\n",
        "    return x['input_ids'].numpy().tolist()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Vnvd4mrtPHHV"
      },
      "source": [
        "## LOAD DATA\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZD8mmZr06hS-"
      },
      "source": [
        "rdrsegmenter = VnCoreNLP(\"vncorenlp/VnCoreNLP-1.1.1.jar\", annotators=\"wseg\", max_heap_size='-Xmx500m') \n",
        "\n",
        "def get_data(raw_text):\n",
        "    x = []\n",
        "    temp_x = raw_text\n",
        "    \n",
        "    for sample in temp_x :        \n",
        "        sample = rdrsegmenter.tokenize(sample) \n",
        "        sample = ' '.join([' '.join(word) for word in sample])\n",
        "        x.append(sample)\n",
        "    x = seq2inds(x)\n",
        "    return x"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jlwrJDOOV2md"
      },
      "source": [
        "Load dữ liệu"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0DTLxoLw62ld"
      },
      "source": [
        "data_train = pickle.load(open('Data/data_train.pkl', 'rb'))\n",
        "data_test = pickle.load(open('Data/data_test.pkl', 'rb'))\n",
        "target_train = pickle.load(open('Data/target_train.pkl', 'rb'))\n",
        "target_test = pickle.load(open('Data/target_test.pkl', 'rb'))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dYLwQlYQ5DE9"
      },
      "source": [
        "x_train= get_data(data_train[:5000])\n",
        "x_test= get_data(data_test)\n",
        "y_train = pd.get_dummies(target_train[:5000]).values.tolist()\n",
        "y_test = pd.get_dummies(target_test).values.tolist()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yMvByBTsdtpF"
      },
      "source": [
        "pickle.dump(x_train, open('PhoBERT/x_train.pkl', 'wb'))\n",
        "pickle.dump(y_train, open('PhoBERT/y_train.pkl', 'wb'))\n",
        "pickle.dump(x_test, open('PhoBERT/x_test.pkl', 'wb'))\n",
        "pickle.dump(y_test, open('PhoBERT/y_test.pkl', 'wb'))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hueN-IoHxOVK"
      },
      "source": [
        "Nếu đã có thì không cần xử lý"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X9e4bjvcxRJr"
      },
      "source": [
        "x_train = pickle.load(open('PhoBERT/x_train.pkl', 'rb'))\n",
        "y_train = pickle.load(open('PhoBERT/y_train.pkl', 'rb'))\n",
        "x_test = pickle.load(open('PhoBERT/x_test.pkl', 'rb'))\n",
        "y_test = pickle.load(open('PhoBERT/y_test.pkl', 'rb'))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d-jQI1wRBJGg"
      },
      "source": [
        "X_train, X_val, Y_train, Y_val = train_test_split(np.array(x_train), np.array(y_train), test_size=0.1, random_state=42)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pDNKfAXbDnJH"
      },
      "source": [
        "## Define MODEL\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Pg1p7xDV9MY9"
      },
      "source": [
        "# Set an optimizer\n",
        "optimizer = Adagrad(\n",
        "    learning_rate=0.001)\n",
        "\n",
        "# Set loss and metrics\n",
        "loss = CategoricalCrossentropy()\n",
        "metric = CategoricalAccuracy('accuracy')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aksj743St9ga"
      },
      "source": [
        "def build_classifier_model():\n",
        "    # Load the MainLayer\n",
        "    bert = transformer_model.layers[0]\n",
        "\n",
        "    # Build your model input\n",
        "    inputs = Input(shape=(MAX_SEQ_LEN,), name='input_ids', dtype='int32')\n",
        "\n",
        "    # Load the Transformers BERT model as a layer in a Keras model\n",
        "    bert_model = bert(inputs)[1]\n",
        "    dropout = Dropout(config.hidden_dropout_prob, name='pooled_output')\n",
        "    pooled_output = dropout(bert_model, training=False)\n",
        "    net = Dense(units=256, activation='relu', name='dense')(pooled_output)\n",
        "    # Then build your model output\n",
        "    outputs = Dense(units=number_topic, activation='softmax', name='classifier')(net)\n",
        "    bert_model.trainable =True\n",
        "    bert.trainable =True\n",
        "    # And combine it all in a model object\n",
        "    model = Model(inputs=inputs, outputs=outputs, name='BERT_MultiLabel_MultiClass')\n",
        "\n",
        "    # Take a look at the model\n",
        "    model.summary()\n",
        "    tf.keras.utils.plot_model(model)\n",
        "    model.compile( optimizer = optimizer, loss = loss, metrics = metric)\n",
        "    return model\n",
        "\n",
        "classifier_model = build_classifier_model()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WbUWoZMwc302"
      },
      "source": [
        "## Model training"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HtfDFAnN_Neu"
      },
      "source": [
        "history = classifier_model.fit(X_train, Y_train, validation_data=(X_val, Y_val), epochs=3, batch_size=16)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KEsxQZDtGv1e"
      },
      "source": [
        "#save model\n",
        "saved_model_path = \"PhoBERT/model/PhoBERT250.save\"\n",
        "# classifier_model.save(saved_model_path)\n",
        "classifier_model = tf.keras.models.load_model(\"PhoBERT200.save\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "LAZY"
      ],
      "metadata": {
        "id": "GMzJTDc-cxFn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def build_lazy_model():\n",
        "    # Load the MainLayer\n",
        "    bert = transformer_model.layers[0]\n",
        "\n",
        "    # Build your model input\n",
        "    inputs = Input(shape=(MAX_SEQ_LEN,), name='input_ids', dtype='int32')\n",
        "\n",
        "    # Load the Transformers BERT model as a layer in a Keras model\n",
        "    bert_model = bert(inputs)[1]\n",
        "    dropout = Dropout(config.hidden_dropout_prob, name='pooled_output')\n",
        "    pooled_output = dropout(bert_model, training=False)\n",
        "    bert_model.trainable =False\n",
        "    bert.trainable =False\n",
        "\n",
        "    net = Dense(units=512, activation='relu', name='dense')(pooled_output)\n",
        "    # Then build your model output\n",
        "    outputs = Dense(units=number_topic, activation='softmax', name='classifier')(net)\n",
        "\n",
        "    # And combine it all in a model object\n",
        "    model = Model(inputs=inputs, outputs=outputs, name='BERT_MultiLabel_MultiClass')\n",
        "\n",
        "    # Take a look at the model\n",
        "    model.summary()\n",
        "    tf.keras.utils.plot_model(model)\n",
        "    model.compile( optimizer = optimizer, loss = loss, metrics = metric)\n",
        "    return model\n",
        "\n",
        "lazy_model = build_lazy_model()"
      ],
      "metadata": {
        "id": "VlUr2W-fczBv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "lazy_history = lazy_model.fit(X_train, Y_train, validation_data=(X_val, Y_val), epochs=5, batch_size=16)"
      ],
      "metadata": {
        "id": "sQXhxR0ifI1-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "history_dict = lazy_history.history"
      ],
      "metadata": {
        "id": "kQApJVhTIxb_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uBthMlTSV8kn"
      },
      "source": [
        "### Evaluate the model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "slqB-urBV9sP"
      },
      "source": [
        "loss, accuracy = classifier_model.evaluate(x_test, y_test)\n",
        "\n",
        "print(f'Loss: {loss}')\n",
        "print(f'Accuracy: {accuracy}')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uttWpgmSfzq9"
      },
      "source": [
        "### Plot the accuracy and loss over time\n",
        "\n",
        "Based on the `History` object returned by `model.fit()`. You can plot the training and validation loss for comparison, as well as the training and validation accuracy:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fiythcODf0xo"
      },
      "source": [
        "history_dict = history.history\n",
        "print(history_dict.keys())\n",
        "\n",
        "acc = history_dict['accuracy']\n",
        "val_acc = history_dict['val_accuracy']\n",
        "loss = history_dict['loss']\n",
        "val_loss = history_dict['val_loss']\n",
        "\n",
        "epochs = range(1, len(acc) + 1)\n",
        "fig = plt.figure(figsize=(10, 6))\n",
        "fig.tight_layout()\n",
        "\n",
        "plt.subplot(2, 1, 1)\n",
        "# \"bo\" is for \"blue dot\"\n",
        "plt.plot(epochs, loss, 'r', label='Training loss')\n",
        "# b is for \"solid blue line\"\n",
        "plt.plot(epochs, val_loss, 'b', label='Validation loss')\n",
        "plt.title('Training and validation loss')\n",
        "# plt.xlabel('Epochs')\n",
        "plt.ylabel('Loss')\n",
        "plt.legend()\n",
        "\n",
        "plt.subplot(2, 1, 2)\n",
        "plt.plot(epochs, acc, 'r', label='Training acc')\n",
        "plt.plot(epochs, val_acc, 'b', label='Validation acc')\n",
        "plt.title('Training and validation accuracy')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.legend(loc='lower right')\n",
        "plt.savefig(\"history.png\")"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}
